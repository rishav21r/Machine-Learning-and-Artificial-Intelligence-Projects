{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":5793796,"sourceType":"datasetVersion","datasetId":199387}],"dockerImageVersionId":30761,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# MapReduce with PySpark\n\nMapReduce is a programming model for processing large datasets with a parallel, distributed algorithm on a cluster. To demonstrate the MapReduce process step-by-step, we'll use a public dataset from Kaggle, such as the \"US Accidents\" dataset, which can be a good example to work with. Here's how you can perform MapReduce operations, typically using Python with the PySpark library, which provides an interface for programming entire clusters with implicit data parallelism and fault tolerance.\n\n## Step 1: Setup and Load Data\nFirst, install PySpark via pip.","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"code","source":"pip install pyspark","metadata":{"execution":{"iopub.status.busy":"2024-08-28T16:59:19.839491Z","iopub.execute_input":"2024-08-28T16:59:19.839929Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Collecting pyspark\n  Downloading pyspark-3.5.2.tar.gz (317.3 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.3/317.3 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}